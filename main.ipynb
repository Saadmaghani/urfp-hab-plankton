{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor\n",
    "from training import Trainer\n",
    "from metrics import Metrics\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models.vgg_TL import GoogleNet\n",
    "from models.autoencoders import Simple_AE\n",
    "from configuration import Hyperparameters as HP\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "\n",
    "years = [str(y) for y in range(2006, 2015)]\n",
    "\n",
    "classes = [\"detritus\", \"Leptocylindrus\", \"Chaetoceros\", \"Rhizosolenia\", \"Guinardia_delicatula\", \"Cerataulina\", \"Cylindrotheca\",\n",
    "    \"Skeletonema\", \"Dactyliosolen\", \"Thalassiosira\", \"Dinobryon\", \"Corethron\", \"Thalassionema\", \"Ditylum\", \"pennate\", \"Prorocentrum\",\n",
    "    \"Pseudonitzschia\", \"Tintinnid\", \"Guinardia_striata\", \"Phaeocystis\"]\n",
    "\n",
    "all_classes = [\"mix\", \"detritus\", \"Leptocylindrus\", \"mix_elongated\", \"Chaetoceros\", \"dino30\", \"Rhizosolenia\", \"Guinardia_delicatula\", \n",
    "\"Cerataulina\", \"Cylindrotheca\", \"Skeletonema\", \"Ciliate_mix\", \"Dactyliosolen\", \"Thalassiosira\", \"bad\", \"Dinobryon\", \"Corethron\", \n",
    "\"DactFragCerataul\", \"Thalassionema\", \"Ditylum\", \"pennate\", \"Prorocentrum\", \"Pseudonitzschia\", \"Mesodinium_sp\", \"G_delicatula_parasite\", \n",
    "\"Tintinnid\", \"Guinardia_striata\", \"Phaeocystis\", \"Dictyocha\", \"Pleurosigma\", \"Eucampia\", \"Thalassiosira_dirty\", \n",
    "\"Asterionellopsis\", \"flagellate_sp3\", \"Laboea_strobila\", \"Chaetoceros_didymus_flagellate\", \"Heterocapsa_triquetra\", \"Guinardia_flaccida\", \n",
    "\"Chaetoceros_pennate\", \"Ceratium\", \"Euglena\", \"Coscinodiscus\", \"Strombidium_morphotype1\", \"Paralia\", \"Gyrodinium\", \"Ephemera\", \"Pyramimonas_longicauda\", \n",
    "\"Proterythropsis_sp\", \"Gonyaulax\", \"kiteflagellates\", \"Chrysochromulina\", \"Chaetoceros_didymus\", \"bead\", \"Katodinium_or_Torodinium\", \"Leptocylindrus_mediterraneus\", \n",
    "\"spore\", \"Tontonia_gracillima\", \"Delphineis\", \"Dinophysis\", \"Strombidium_morphotype2\", \"Licmophora\", \"Lauderia\", \"clusterflagellate\", \"Strobilidium_morphotype1\", \n",
    "\"Leegaardiella_ovalis\", \"pennate_morphotype1\", \"amoeba\", \"Strombidium_inclinatum\", \"Pseudochattonella_farcimen\", \"Amphidinium_sp\", \"dino_large1\", \n",
    "\"Strombidium_wulffi\", \"Chaetoceros_flagellate\", \"Strombidium_oculatum\", \"Cerataulina_flagellate\", \"Emiliania_huxleyi\", \"Pleuronema_sp\", \"Strombidium_conicum\",\n",
    " \"Odontella\", \"Protoperidinium\", \"zooplankton\", \"Stephanopyxis\", \"Tontonia_appendiculariformis\", \"Strombidium_capitatum\", \"Bidulphia\", \"Euplotes_sp\", \n",
    " \"Parvicorbicula_socialis\", \"bubble\", \"Hemiaulus\", \"Didinium_sp\", \"pollen\", \"Tiarina_fusus\", \"Bacillaria\", \"Cochlodinium\", \"Akashiwo\", \"Karenia\"]\n",
    "\n",
    "classes_30 = [\"Asterionellopsis\", \"bad\", \"Chaetoceros\", \"Chaetoceros_flagellate\", \"Ciliate_mix\", \"Corethron\", \"Cylindrotheca\", \"Dictyocha\",\"dino30\", \"detritus\",\n",
    "\t\"Dinobryon\", \"Ditylum\", \"Eucampia\", \"flagellate_sp3\", \"Guinardia_delicatula\", \"Guinardia_flaccida\", \"Guinardia_striata\", \"Heterocapsa_triquetra\", \"Laboea_strobila\", \"Leptocylindrus\",\n",
    "\t\"pennate\", \"Phaeocystis\", \"Pleurosigma\", \"Prorocentrum\", \"Pseudonitzschia\", \"Skeletonema\", \"Thalassionema\", \"Thalassiosira\", \"Thalassiosira_dirty\", \"Tintinnid\"]\n",
    "\n",
    "print(len(classes_30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pp = Preprocessor(years, include_classes=classes, train_eg_per_class=HP.number_of_images_per_class)\n",
    "#pp = Preprocessor(years, include_classes=all_classes, train_eg_per_class=HP.number_of_images_per_class, thresholding=HP.thresholding)\n",
    "pp = Preprocessor(years, include_classes=classes_30, strategy = HP.pp_strategy, train_eg_per_class=HP.number_of_images_per_class, maxN = HP.maxN, minimum = HP.minimum, transformations = HP.transformations)\n",
    "\n",
    "\n",
    "pp.create_datasets([0.6,0.2,0.2])\n",
    "\n",
    "trainLoader = pp.get_loaders('train', HP.batch_size)\n",
    "validLoader = pp.get_loaders('validation', HP.batch_size)\n",
    "testLoader = pp.get_loaders('test', HP.batch_size)\n",
    "\n",
    "\n",
    "trainer = Trainer(HP_version = HP.version, epochs = HP.number_of_epochs, loss_fn = HP.loss_function, optimizer = HP.optimizer, \n",
    "\tscheduler = HP.scheduler, lr = HP.learning_rate, momentum = HP.momentum, useCuda=True, autoencoder=HP.train_AE)\n",
    "\n",
    "\n",
    "# using autoencoder\n",
    "ae = Simple_AE()\n",
    "path_to_ae = \"../Simple_AE_3.0-10.1.pth\"\n",
    "\n",
    "if \".tar\" in path_to_ae:\n",
    "    ae = trainer.load_partial_model(ae, path_to_ae)\n",
    "else:\n",
    "    ae = trainer.load_full_model(ae, path_to_ae)\n",
    "\n",
    "model = GoogleNet(autoencoder = ae)\n",
    "\n",
    "\n",
    "trainAcc = []\n",
    "validAcc = [] \n",
    "epochs = 0 \n",
    "\n",
    "trainAcc, validAcc, epochs = trainer.train(model, trainLoader, validLoader, earlyStopping = HP.es)\n",
    "\n",
    "# - or -\n",
    "\"\"\"\n",
    "path_to_statedict = \"models/GoogleNet_2.0-8.0.tar\"\n",
    "\n",
    "if \".tar\" in path_to_statedict:\n",
    "    model = trainer.load_partial_model(model, path_to_statedict)\n",
    "else:\n",
    "    model = trainer.load_full_model(model, path_to_statedict)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# autoencoder stuff:\n",
    "\"\"\"\n",
    "test_sumsqs, test_fnames = trainer.test_autoencoder(model, testLoader)\n",
    "test_acc = torch.mean(test_sumsqs).tolist()\n",
    "\"\"\"\n",
    "\n",
    "test_pred, test_target, test_fnames = trainer.test(model, testLoader)\n",
    "#valid_pred, valid_target, valid_fnames = trainer.test(model, validLoader)\n",
    "#train_pred, train_target, train_fnames = trainer.test(model, trainLoader)\n",
    "\n",
    "\n",
    "test_met = Metrics(test_target, test_pred)\n",
    "#valid_met = Metrics(valid_target, valid_pred)\n",
    "#train_met = Metrics(train_target, train_pred)\n",
    "\n",
    "test_acc = test_met.accuracy()\n",
    "print(test_met.accuracy())\n",
    "\n",
    "print(test_acc)\n",
    "\n",
    "#time = trainer.getTime()\n",
    "time = \"xx:xx:xx\"\n",
    "print(time)\n",
    "\n",
    "f = open(\"./stats/stats-\"+str(model)+\"-\"+str(HP.version)+\".json\",\"w+\")\n",
    "\n",
    "#str(test_met.accuracy()) + \\\n",
    "\n",
    "str_to_write = \"{\\\"Time\\\": \\\"\"+ time +\"\\\",\\n \\\"Epochs\\\": \"+str(epochs)+ \",\\n \\\"TrainAcc\\\": \"+ str(trainAcc)+\",\\n \\\"ValidAcc\\\": \"+str(validAcc)+\",\\n \\\"TestAcc\\\": \"+ str(test_acc) + \"}\"\n",
    "#\",\\n \\\"Train_Pred\\\": \" + str(list(train_pred.cpu().numpy())) + \",\\n \\\"Train_Target\\\": \" + str(list(train_target.cpu().numpy())) + \",\\n \\\"Train_fnames\\\": \" + json.dumps(train_fnames) + \\\n",
    "#\",\\n \\\"Valid_Pred\\\": \" + str(list(valid_pred.cpu().numpy())) + \",\\n \\\"Valid_Target\\\": \" + str(list(valid_target.cpu().numpy())) + \",\\n \\\"Valid_fnames\\\": \" + json.dumps(valid_fnames) + \\\n",
    "#\",\\n \\\"Test_Pred\\\": \" + str(list(test_pred.cpu().numpy())) + \",\\n \\\"Test_Target\\\": \" + str(list(test_target.cpu().numpy())) + \",\\n \\\"Test_fnames\\\": \" + json.dumps(test_fnames) + \\\n",
    "#\"}\"\n",
    "f.write(str_to_write)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GoogleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725838\n",
      "66211\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor(years, include_classes=classes_30, strategy = HP.pp_strategy, train_eg_per_class=HP.number_of_images_per_class, maxN = HP.maxN, minimum = HP.minimum, transformations = HP.transformations)\n",
    "pp.create_datasets([0.6,0.2,0.2])\n",
    "\n",
    "trainLoader = pp.get_loaders('train', HP.batch_size)\n",
    "validLoader = pp.get_loaders('validation', HP.batch_size)\n",
    "testLoader = pp.get_loaders('test', HP.batch_size)\n",
    "\n",
    "trainer = Trainer(HP_version = HP.version, epochs = HP.number_of_epochs, loss_fn = HP.loss_function, optimizer = HP.optimizer, \n",
    "\tscheduler = HP.scheduler, lr = HP.learning_rate, momentum = HP.momentum, useCuda=True, autoencoder=HP.train_AE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/30/smaghani/URFP/models/vgg_TL.py:179: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02839235822698784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02839235822698784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred, test_target, test_fnames = trainer.test(model, testLoader)\n",
    "test_met = Metrics(test_target, test_pred)\n",
    "test_met.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02809243316719529\n",
      "0.028696571514876906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.028696571514876906"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pred, valid_target, valid_fnames = trainer.test(model, validLoader)\n",
    "train_pred, train_target, train_fnames = trainer.test(model, trainLoader)\n",
    "valid_met = Metrics(valid_target, valid_pred)\n",
    "train_met = Metrics(train_target, train_pred)\n",
    "valid_met.accuracy()\n",
    "train_met.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725838\n",
      "66211\n",
      "0.0318658914143321\n",
      "0.034964506872073704\n",
      "0.033277953984795855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.033277953984795855"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from preprocessing import Rescale, RandomCrop, ToTensor\n",
    "\n",
    "pp = Preprocessor(years, include_classes=classes_30, strategy = HP.pp_strategy, train_eg_per_class=HP.number_of_images_per_class, maxN = HP.maxN, minimum = HP.minimum, transformations = transforms.Compose([Rescale((64, 128)), ToTensor()]))\n",
    "pp.create_datasets([0.6,0.2,0.2])\n",
    "\n",
    "trainLoader = pp.get_loaders('train', HP.batch_size)\n",
    "validLoader = pp.get_loaders('validation', HP.batch_size)\n",
    "testLoader = pp.get_loaders('test', HP.batch_size)\n",
    "\n",
    "trainer = Trainer(HP_version = HP.version, epochs = HP.number_of_epochs, loss_fn = HP.loss_function, optimizer = HP.optimizer, \n",
    "\tscheduler = HP.scheduler, lr = HP.learning_rate, momentum = HP.momentum, useCuda=True, autoencoder=HP.train_AE)\n",
    "\n",
    "\n",
    "test_pred, test_target, test_fnames = trainer.test(model, testLoader)\n",
    "valid_pred, valid_target, valid_fnames = trainer.test(model, validLoader)\n",
    "train_pred, train_target, train_fnames = trainer.test(model, trainLoader)\n",
    "test_met = Metrics(test_target, test_pred)\n",
    "valid_met = Metrics(valid_target, valid_pred)\n",
    "train_met = Metrics(train_target, train_pred)\n",
    "test_met.accuracy()\n",
    "valid_met.accuracy()\n",
    "train_met.accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725838\n",
      "66211\n",
      "0.02733519595257872\n",
      "0.026733121884911646\n",
      "0.02617932839953683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02617932839953683"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from preprocessing import Rescale, RandomCrop, ToTensor\n",
    "\n",
    "pp = Preprocessor(years, include_classes=classes_30, strategy = HP.pp_strategy, train_eg_per_class=HP.number_of_images_per_class, maxN = HP.maxN, minimum = HP.minimum, transformations = transforms.Compose([Rescale((32, 64)), ToTensor()]))\n",
    "pp.create_datasets([0.6,0.2,0.2])\n",
    "\n",
    "trainLoader = pp.get_loaders('train', HP.batch_size)\n",
    "validLoader = pp.get_loaders('validation', HP.batch_size)\n",
    "testLoader = pp.get_loaders('test', HP.batch_size)\n",
    "\n",
    "trainer = Trainer(HP_version = HP.version, epochs = HP.number_of_epochs, loss_fn = HP.loss_function, optimizer = HP.optimizer, \n",
    "\tscheduler = HP.scheduler, lr = HP.learning_rate, momentum = HP.momentum, useCuda=True, autoencoder=HP.train_AE)\n",
    "\n",
    "\n",
    "test_pred, test_target, test_fnames = trainer.test(model, testLoader)\n",
    "valid_pred, valid_target, valid_fnames = trainer.test(model, validLoader)\n",
    "train_pred, train_target, train_fnames = trainer.test(model, trainLoader)\n",
    "test_met = Metrics(test_target, test_pred)\n",
    "valid_met = Metrics(valid_target, valid_pred)\n",
    "train_met = Metrics(train_target, train_pred)\n",
    "test_met.accuracy()\n",
    "valid_met.accuracy()\n",
    "train_met.accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from preprocessing import Rescale, RandomCrop, ToTensor\n",
    "\n",
    "model = GoogleNet()\n",
    "\n",
    "pp = Preprocessor(years, include_classes=classes_30, strategy = HP.pp_strategy, train_eg_per_class=HP.number_of_images_per_class, maxN = HP.maxN, minimum = HP.minimum, transformations = transforms.Compose([Rescale((256, 256)), ToTensor()]))\n",
    "pp.create_datasets([0.6,0.2,0.2])\n",
    "\n",
    "trainLoader = pp.get_loaders('train', HP.batch_size)\n",
    "validLoader = pp.get_loaders('validation', HP.batch_size)\n",
    "testLoader = pp.get_loaders('test', HP.batch_size)\n",
    "\n",
    "trainer = Trainer(HP_version = HP.version, epochs = HP.number_of_epochs, loss_fn = HP.loss_function, optimizer = HP.optimizer, \n",
    "\tscheduler = HP.scheduler, lr = HP.learning_rate, momentum = HP.momentum, useCuda=True, autoencoder=HP.train_AE)\n",
    "\n",
    "\n",
    "test_pred, test_target, test_fnames = trainer.test(model, testLoader)\n",
    "valid_pred, valid_target, valid_fnames = trainer.test(model, validLoader)\n",
    "train_pred, train_target, train_fnames = trainer.test(model, trainLoader)\n",
    "test_met = Metrics(test_target, test_pred)\n",
    "valid_met = Metrics(valid_target, valid_pred)\n",
    "train_met = Metrics(train_target, train_pred)\n",
    "test_met.accuracy()\n",
    "valid_met.accuracy()\n",
    "train_met.accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
