3/7 - wednesday
1.	the 103 categories of WHOI plankton dataset are not a super category of the 82 special planktons of the algae which cause red tides (taken from hk gov site). There are only 2 which match in exact name, 
	44 which match the first name (guessing thats a general name for type of plankton) and 36 of the remaining 82 which dont match at all.
	Idealy all 82 categories would match one category of the 103. ie. the 82 would be a subset of the 103.


4/7 - thursday
1.	regarding 1 of 3/7:
	a)	i started to find out what exactly the naming system is. i am using marinespecies.org to find out the meaning. 
		basically the 82 names are species names whereas the 103 are mostly genus names. therefore i have started to list which of the partial matches 
		have how many species in each genus. maybe only one species for a particular genus will lead to good result
		results: only akashiwo genus had 1 child. the rest had more and thus completely useless. but we can change yes to 3 and partially to 43 yay lol
	b)	find another dataset of pics which give species
	c)	will find synonymous names to the species names. maybe of the 'no's there are matches when put to synonymous names. uhhh doubt it though.


5/7 - friday
1. 	regarding 2 of 4/7: 
	a) 	of downloaded multiple datasets. none seem to be promising however, online resources seem to be promising
			1- https://ecotaxa.obs-vlfr.fr/explore/
			2- https://www.st.nmfs.noaa.gov/nauplius/media/copepedia/taxa/T2000000/html/photoframe.html
			3- http://www.ices.dk/marine-data/dataset-collections/Pages/Plankton.aspx
			4- https://planktonnet.awi.de/#search
	b) 	3 does not have images. 2 does not have enough images. 1 does not have images to the species degree which i am looking for. 
		4 i have emailed the PI and hopefully can download the entire dataset of images. however images are varied and will require a lot of preprocessing in order to be helpful for the ML task
2. 	regarding 3 of 4/7:
	a) I have not taken this approach yet because its unlikely that it will yield good results
3. 	another approach to solve 1 of 3/7 is to tune the problem to match the data. the WHOI dataset is VERY good but it does not solve the problem at hand and most of the datasets dont. 
	therefore maybe we change the problem? predict a few species which cause HAB? of the WHOI dataset only 3 species exactly match and 43 which partially cause. maybe change the problem
	to further classify the species given genus data. 
4. 	another approach - find a larger set of algal species which cause HAB rather than just the 82 mentioned... Will try this next!


6/7 - Saturday
1. 	regarding 4 of 5/7:
	a) 	looking at resources which gives more information about algae which cause HAB:
	 		1- https://www.whoi.edu/website/redtide/species/by-name/
	 		2- http://www.marinespecies.org/hab/aphia.php?p=taxlist
		certain genus are harmful, without exception, and therefore if we can detect the genus then we dont have to further specify the species. 
		will take note of which genus are harmful on spreadsheet.
	b)	1 is a subset of 2. therefore i took all accepted algae of 2 (311) and added it to the 82 species which cause HAB.
	c) 	now I have 11 algae which can be matched from the WHOI dataset from algae of 2 and 3 which can be matched from the 82. so total of 14
2. 	because I have 14 things which I can classify I will proceed to the next step but I have to talk with Zhiming (or Prof) about this dataset issue
	Next steps:
		a) 	data exploration with regards to the 14 things
		b)	pipeline. as of now this is what I have. Raw Data -> Data augmentation -> Auto-encoder -> CNN
		c) 	literature review
		d) 	hyper-parameters of CNN and Auto-encoder (get from literature review)
	Will start with data exploration today


8/7 - Monday
1.	Data exploration:
	a)	new document - data_exploration.txt
	b) 	make a python script (data_exploration.py) which gets all the #of pics per year per category (15)
	c) 	downloading all datasets on chiwah computer and then getting all the statistics (count). not a good way cuz chiwah net fast but computer slow.
		would rather have access to a better computer (maybe in a lab? lol). ASK ZHIMING ABOUT THIS
2. 	Literature review:
	a)	new google document - literature review
	b) 	references:
			1- https://towardsdatascience.com/deep-blue-sea-using-deep-learning-to-detect-hundreds-of-different-plankton-species-dff895d3b226 (kaggle NDSB challenge)
			2- http://cs231n.stanford.edu/reports/2015/pdfs/Dangelo_Tennefoss_Final_paper.pdf (terrible)
			3- http://mmlab.ie.cuhk.edu.hk/archive/2006/JOE06_plankton.pdf (doesnt use NN)
			4- http://vincent-net.com/luc/papers/98air_plankton.pdf (insane one)
			5- https://www-sciencedirect-com.eproxy.lib.hku.hk/science/article/pii/S156898830600031X (didnt look into)
			6- https://www2.whoi.edu/staff/hsosik/wp-content/uploads/sites/11/2017/03/Campbell_etal_EnvironSciPollutResInt2013.pdf (didnt look into)
			7- https://pdfs.semanticscholar.org/ed26/f44893b2e53147ca86b4e7bfaa1eeeb9832f.pdf (2 phase - 1 cnn 2 cnn)
			8- https://ieeexplore-ieee-org.eproxy.lib.hku.hk/stamp/stamp.jsp?tp=&arnumber=7533053 (thresholding)
	c) 	Some questions to ask zhiming in literature review doc


9/7 - Tuesday
1. 	regarding to 1c of 8/7: 
		shifted work to GPU cluster. downloaded all datasets and started to unzip them. might need more space and time
2.	continued work on literature review. will add links to the papers I am reviewing


10/7 - wednesday
1. 	continued to work on literature review
	a)	will look into Hintons capsules as possible new architecture - get zhiming advice
	b) 	will look into papers which use WHOI dataset
2.	unziping datasets and upd`ating numbers on google sheets
3.	meeting with zheming:
	a)	VGG, Googlenet and resnet to extract features
	b)	JUST DO IT


11/7 - thursday
1.	worked on data exploration

13/7 - saturday
1. 	started learning pytorch
2.	will make a simple CNN classifier by the end of today (Insha Allah)
3.	new google document - implementation

14/7 - Sunday
1.	regarding 2 of 13/7, made a simple CNN from tutorial. now will work on using it for my dataset
2.	added 'dimensions' to list of stats which we need. modifying data_exploration.py to get dimensions

15/7 - monday
1.	hopefully will finish up first CNN today. shifted jupyter notebook on gpu-cluster

17/7 - wednesday
1.	regarding 1. of 15/7, havent finished CNN. problems faced is structuring the dataset

18/7 - thursday
1.	NEED TO GET THE MDOEL RUNNING MY GOD

19/7 - thursday
1.	AAAHHH
2.	started working on making the entire structure so that it is modular. new files mentioned in implementation doc

21/7 - sunday
1.	finished with preprocessing.py

22/7 - monday
1.	finished training.py and main.ipynb
2. 	will push all work to github and then pull from gpu cluster and start training

24/7 - wednesday
1.	met with Zhiming, new architecture (version 2) and lots of tips.
2. 	now need to train it and see if it helps.

26/7 - friday
1.	ran on 500 image data sets / class, total of 10000 images. accuracy low - 5%
1.	ran out of GPU time, applied for more

29/7 - monday
1.	got GPU time. training. approx will take 6 hours for 10000 data. 
2.	another file - configuration.py, has all the hyperparameters and hyperparameter version. 
3.	45% on model 2.2 HP 2.0

1/8 - thursday 
1. 	got gpu time. training with model 2.3 HP 2.0 - test accuracy 36%
2. 	implemented early stopping, trying with 1000 images and early stopping (epochs = 200) 
	1.	33% accuracy with model 2.3 HP 3.0
3. 	going to train with model 2.2 HP 3.0
4.	training with HP 3.1 - minibatch = 256

2/8 - friday
1.	70% with model 2.2 HP 3.1
2.	plotting train, valid accuracies and will see whats the issues
3.	train 2.2/3.1 more as 2.2/3.2 = 75% test accuracy plotting train/valid accuracies
4.	train 2.2/3.1 more as 2.2/3.3 with patience = 20; test acc = 77%


9/8 - Friday
1.	change training:
	1.	to include getting best weights over epochs
	2.	to include a scheduler which would decrease Learning Rate over time
	3.	to test against test data and validation data after X minibatches (X = 50) 

10/8 - Saturday
1.	added seeds to getting data from raw data and train/valid/test splits
2. 	added TL vgg16 model. will test against vgg16 bn and others. need to do more literature review to figure out best tl model.
3.	82% with vgg v1.0. lets plot it -> training was increasing but validation wasnt. epochs for 21.
4.	vgg_bn = 1.1; lets train it
5.	next I should try to see what happens if i use it as a feature extractor, increase size of dataset, change model (vgg)

11/8 - Sunday
1.	86.9% with vgg16_bn; now trying vgg_bn as feature extractor = 66.89%
2.	vgg16 as feature extractor = 33.3%
3.	so feature extractor = bad. lets try vgg11_bn?

12/8 - Monday
1.	vg11_bn = 85.8%. vg19_bn? 
2.	vgg19_bn = 89.15% (1.5/3.5); im guessing vgg19 is less but lets do it. vgg_19 = 86.35%
3.	so with all of them, early stopping is working but its weird. i think i have to train it more.
4.	going to print out validation and train acc more frequently (every 10)
5.	IDEAS:
	1.	to reflect the actual distribution of classes, I will take unnormalized classes and predict on them. A paper does 
		transfer learning on normalized classes and then fine tunes it on unnormalized classes. I could TL from vgg19_bn -> normalized 
		data -> unnormalized data then test it on unnormalized data. - ASK ZHIMING
	2. 	increase output classes to all 103? 
		near impossible as some have literally 4. could have a cut off at 100 meaning we can predict 77 classes. we could then use data augmentation
	3.	Incorporate "thresholding" which means to take at most N images of a particular class. so you could have <N images of a class too
	4.	Instead of making the data normalized (ie. all classes have equal images), split each class into train/valid/test. so dont split over entire
		dataset rather split over each class. (lets implement this)

13/8 - Tuesday
1.	Result of vgg19 with 40 patience = 81% so decrease patience to 20 and keep it vgg19_bn
2.	taking zhiming advice - copying input to 3 channels. - 89.45% (1.7/3.5)
3.	increasing to 1000 images - 1.7/3.7 - 92%

14/8 - Wednesday
1.	will take in all 103 classes right now and train it.

16/8 - Friday
1.	1. of 14/8 failed because it was taking toooo long (8 hours and 1 epoch was not done).
2.  According to one paper I should fine tune only certain layers and freeze the other layers. Also we can try different models (like WideNet)
	Lets incrementally find out (while i program other shiz)
    a.	Wide ResNet-101-2 w/ 3.7 = 91.625
    b.	AlexNet w/ 3.7 = 86.5
    c.	according to one paper [http://vashishtm.net/pdf/practices-fine-tuning.pdf] you should fine tune half and randomly initialize other half for
    	best results. so im gonna do that 
    		1 - 0.922
    		2 - 

17/8 - Saturday
1.	ran out of gpu time.

19/8 - Monday
1.	implementing thresholding.


23/8 - Friday
1.  Didnt do anything the entire week, just got GPU rss approved. GOTTA DO SOMETHING. Very limited time, i want to do 16/8 - 1c.
    Then I will do 19/8 - 1 with the best result of 16/8 - 1c.
2.  restarting 16/8 1c.


25/8 - Sunday
1.	GoogleNet (3.7) - 93.05% best result yet
2.	lets do thresholding with googlenet
3.	thresholding with googlenet taking long time. doing resnet18, resnet50 with 3.7
4.	talked to zhiming:
	1.	find out backbone NN (done)
	2.	apply some idea using backbone NN to get higher accuracy/f-measure
	3.	share onenote with him (done)
	4.	read cvpr iccv eccv conference papers
	5.	email professors about phd (look at chinese U and UST professors too)
	6.	email Ping Lou about working as an undergraduate RA and then about PhD
	7.	include more images (done) 
	

2/9 - Monday
Stastics of each category - accuracy
czm
czm


week of 2/9:
1. talked to zhiming and need to introduce statistics of each category. lets do that today (8/9)
2. trained some models:
	1. GoogleNet 1.2:
		- 4.0 = 93.95 (7,50 hours)
		- 4.1 = 94.04 (11,45 hours)
		- 5.0 = 93.08 (2,30 hours)
		- 5.1 = 94.00 (13,30 hours)
		- 5.2 = 94.75 (??) (model file located in czm server)
		- 5.30 = 90.83, 91.32% (12/12 test)
		- 6.0 = 90.44
		- 6.1 = 90.59
		- 6.2 = 89.74
		- 6.3 = 90.79
		- 6.4 = 89.57
		- 6.5 = 90.75
		- 7.1 = 92.62
	2. GoogleNet 4.0
		- 11.0 (Autoencoder 3.0) = 85
 
data distribution -> class distribution. maybe we should make a table out of it. okay lol lets do that.
shape/appearance of classes not consistent
1.	choose sample of misclassified images
preprocessing
1.	resizing 
2. 	noisy images we could use autoencode 
classification tree after googlenet 

accuracies of training/validation/test sets of each class (done)


24/9:
1. 	had meeting with wenping, twas goood. he liked my passion "want to publish paper", "bigger goal of impact"
2.	zheming - augment classes with less than threshold amount. lets do it but first lets see the training accuracies
3.	we should apply transfer learning from GN -> WHOI -> HKUST database


26/9:
1.	first with images Tn >, include all images or up till Tn. find what are good Tn values
2.	Cross Validation

29/9:
1.	to do time-series analysis lets plot the different classes per day/year. so we have an idea of its increase/decrease ie. trend
	to do that:
		1.	year -> class -> get all data -> sort by date then time then order in ascending order
			001_0500_1015 -> 001_0500_1016 -> 001_1030_1001 -> 201_0000_0001 
			to do this we need to sort strings. awk?

1/10:
1.	plotted time-series data of 30 classes per time and per day
2.	looking for database with both plankton count/day|time unit and HAB yes or no

4/10:
1.	still looking for databases, emailed afcd of hk govt and MVCO regarding access to databases
2.	decision on using minimum: << incomplete >>

7/10:
1.	Data augmentation time.
2.	meeting with zhiming and friend. came up with following strategies:
	1.	16 random crops -> 16 outputs, average output (done. pretty bad. why?)
	2.	16 random crops -> gn output (16, 512, 2, 2) -> make into vector -> fc gives output
	3.	attention to get weighted average
	4.	multi-model
	5.	SENET


16/10:
1.	oof still havent fixed data augmentation. im doing it now. fixed i think. 
2.	running 7.1 
3.	one problem is that the test set, though same size, is different every time. i think.
	1.	solved. now everything is exactly the same given #images. so if #images same (say min. max. same) then the dataset will be same too.
4.	running 8.0 on gpu farm
	just a test of focal loss + thresholding vs thresholding w/o focal loss

25/10:
1.	GN 2.0/8.0 took 3 days + and is only at 53% valid accuracy. so im quitting it. taking less data. i need to meet with zhiming too.

26/10:
1. 	models:
	1. GN_2.0/8.0 = 54.00%
	2. GN_2.0/8.1 = 48.47%
	3. GN_3.0/9.0 = very bad. almost a week since training and has not finished. valid acc = 0.033 ie. 3%


6/11:
1. 	the above models show that it leads to very bad results if you use random cropping.
2. 	to introduce variability, lets try to use autoencoders. there are 2 autoencoders i will look at:
	1.	simple autoencoders:
		1.	depth of 1, 2 and 3 (for both encoding and decoding) and then using least dimension vector, i will push it to the Google Net
		2.	with powers of 3 because the input channel is 3 channels. so 3 <-> 27 <-> 9 <-> 3 . if add more then 3 <-> 81 <-> 27 -> ...
	2.	variable autoencoders. 
3.	right now i will make a simple autoencoder
4.	the autoencoder will go from (128, 256) -> (64, 128) -> (128, 256)


7/12:
1.	after a long ass break its time to get bakc on the road. HYPERPARAMETER TUNING OPTIMIZATION. i should have done this a long time ago.
	1. looking at TUNE - https://towardsdatascience.com/fast-hyperparameter-tuning-at-scale-d428223b081c
	
12/12:
1.	oof where does the time fly by
2.	further training of Autoencoder 3.0-10.1 for another 200 epochs. => 7474.27 (loss). really good but it was trained incorrectly (:
3.	training 5.3.0 finally. => 91.32%

16/12:
1.	so lets first start training the autoencoder properly. Another thing to consider is whether we should rescale to (128, 256) or (256, 512) cuz 
	reductions will be (32, 64) vs (64, 128) and our GoogleNet works on (64,128). lets do a simple test what does GN do best on. 
2. 	secondly we should make our VAE.

17/12:
1.	so I tried to do a really simple test on GN for teh image sizes but it didnt work out. therefore a test is to take 200 of each so 6000 images 
	in total and train GN based on all the different tests. tests of (32, 64), (64,128), (128, 256), (256, 256). then take highest acc.
	done on gpu farm
	(32, 64) - 78.15%, 36 mins
	(64, 128) - 85.79%, 16 mins
	(128, 256) -
2.	going to train different models and take average. 